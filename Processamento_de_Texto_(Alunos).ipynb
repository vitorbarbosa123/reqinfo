{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitorbarbosa123/reqinfo/blob/master/Processamento_de_Texto_(Alunos).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRM4kEujWfvh"
      },
      "source": [
        "# Processamento de Texto\n",
        "\n",
        "Nesse notebook, nós vamos ver na prática como podemos aplicar algumas técnicas de processamento de texto programaticamente, usando algumas bibliotecas conhecidas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK"
      ],
      "metadata": {
        "id": "eY-6nQzpHdyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos começar usando a biblioteca  [NLTK (Natural Language Toolkit)](https://www.nltk.org/)."
      ],
      "metadata": {
        "id": "O_QLfxFRHX4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primeiramente, vamos importar as bibliotecas python que vamos usar nesse tutorial:"
      ],
      "metadata": {
        "id": "wCkjpU7yGbPy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ5lxzoWTTkH"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSEiZiM-WyDH"
      },
      "source": [
        "e vamos fazer o download de alguns módulos específicos do NLTK:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oovkh3xLWVob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13b5953-f4de-41a0-c191-530f9d923fee"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTvWr6xRTmlt"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq1pPGC5UUPC"
      },
      "source": [
        "#### Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN91rVCVTjEe"
      },
      "source": [
        "text = \"\"\"Hello Mr. Smith, how are you doing today?\n",
        "    The weather is great, and city is awesome.\n",
        "    The sky is pinkish-blue. You shouldn't eat cardboard\n",
        "\"\"\"\n",
        "\n",
        "tokenized_sent = sent_tokenize(text)\n",
        "print(tokenized_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_Mjh26VvwoI"
      },
      "source": [
        "Nós também podemos tokenizar outras linguas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppCFC0KivJFB"
      },
      "source": [
        "portuguese_text = \"Bom dia, Sr. Smith. Como você está? O tempo está bom, e a cidade maravilhosa.\"\n",
        "\n",
        "print(sent_tokenize(portuguese_text, \"portuguese\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC8SE9fOUW5m"
      },
      "source": [
        "#### Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKu1A9cyT6UA"
      },
      "source": [
        "tokenized_word = word_tokenize(text)\n",
        "print(tokenized_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No exemplo acima, a tokenização de palavras foi aplicada ao texto inteiro. Porém, normalmente, o fluxo adotado é aplicar a tokenização nas sentenças e, em seguida, aplicar nas palavras de cada sentença."
      ],
      "metadata": {
        "id": "W13RA9WxEuZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remoção de Pontuação\n",
        "\n",
        "Remova a pontuação das sentenças 'tokenizadas', antes de aplicar o tekenizador de palavras.\n",
        "\n",
        "**Veja exemplos abaixo:**"
      ],
      "metadata": {
        "id": "gpateeX66-kw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXEMPLO 1**\n",
        "\n",
        "Um dos [tipos de tokenizer oferecidos pelo NLTK](http://www.nltk.org/api/nltk.tokenize.html) é baseado em Expressões Regulares (Regex).\n",
        "Então, por exemplo, você pode definir um tokenizer que detecta sequências de caracteres alfanuméricos como tokens e descarta todo o resto:"
      ],
      "metadata": {
        "id": "DvoZgrQFCW1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "text = \"\"\"Hello Mr. Smith, how are you doing today?\n",
        "    Here the weather is great, the temperature is 25 degrees celsius and today's IR Lab is awesome.\n",
        "    You should come and enjoy this joy-filled environment!\n",
        "\"\"\"\n",
        "\n",
        "tokenized_sent = sent_tokenize(text)\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+') #ou r'[a-zA-Z]+' se não quiser incluir números e underscore, pois \\w+ equivale a [a-zA-Z0-9_]+\n",
        "for sent in tokenized_sent:\n",
        "  sent_without_punct = tokenizer.tokenize(sent)\n",
        "  print(sent_without_punct)"
      ],
      "metadata": {
        "id": "jlPRQUMgBwr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXEMPLO 2**\n",
        "\n",
        "Ou dessa outra forma, em puro python:"
      ],
      "metadata": {
        "id": "aJUFdwQxDw3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "text = \"\"\"Hello Mr. Smith, how are you doing today?\n",
        "    Here the weather is great, the temperature is 25 degrees celsius and today's IR Lab is awesome.\n",
        "    You should come and enjoy this joy-filled environment!\n",
        "\"\"\"\n",
        "unicode_translate_table = dict((ord(char), None) for char in string.punctuation)\n",
        "\n",
        "tokenized_sent = sent_tokenize(text)\n",
        "\n",
        "for sent in tokenized_sent:\n",
        "  sent_without_punct = sent.translate(unicode_translate_table)\n",
        "  print(sent_without_punct)"
      ],
      "metadata": {
        "id": "lAWhhJimDIX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXEMPLO 3**\n",
        "\n",
        "Uma outra alternativa é usar `isalpha()` ou `isalnum()`. Neste caso, você deve aplicar depois da tokenização de palavras, para remover as que não se adequem."
      ],
      "metadata": {
        "id": "XzlSatg8D1g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Hello Mr. Smith, how are you doing today?\n",
        "    Here the weather is great, the temperature is 25 degrees celsius and today's IR Lab is awesome.\n",
        "    You should come and enjoy this joy-filled environment!\n",
        "\"\"\"\n",
        "\n",
        "tokens = [word.lower() for sent in sent_tokenize(text) for word in word_tokenize(sent) if word.isalnum()] #também converte para lowercase\n",
        "print(*tokens)"
      ],
      "metadata": {
        "id": "muBw507rD5gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expanção das contrações (contractions)\n"
      ],
      "metadata": {
        "id": "8ATx-MLUOBQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "yD23HQ6WN4e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "\n",
        "contractions.fix(\"I'll show you a simple example, it's easy to understand. \\\n",
        "I don't wanna show anything else because I'm lazy. I'm gonna stop writing now. \\\n",
        "I'd love to stay, but I've got to go! I gotta go! Shouldn't you go too? \\\n",
        "No, I know you are enjoying today's IR Lab!!\")"
      ],
      "metadata": {
        "id": "NjvzWlkANjAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piaQxNKNU1iK"
      },
      "source": [
        "### Frequency Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eeQxpDbUlgY"
      },
      "source": [
        "fdist = FreqDist(tokenized_word)\n",
        "print(fdist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSb_NDcQtZyk"
      },
      "source": [
        "fdist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daJrFuZ1VL97"
      },
      "source": [
        "fdist.most_common(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVWsTFL4VSwB"
      },
      "source": [
        "fdist.plot(30, cumulative=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7FppDhwVx_d"
      },
      "source": [
        "### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9miqaLILVjKz"
      },
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: A partir da lista de palavras tokenizadas acima, gere uma nova lista de palavras que não contém stop words (use list  comprehension)"
      ],
      "metadata": {
        "id": "ITwOW8B3ezJ7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2abEeQF8moH6"
      },
      "source": [
        "filtered_words =  #seu código aqui\n",
        "\n",
        "print(\"Tokenized Words:\", tokenized_word)\n",
        "print(\"Filterd Sentence:\", filtered_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCVPWBAgoLLn"
      },
      "source": [
        "### Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvVgjJh1oRwo"
      },
      "source": [
        "A **Stemming** reduz as palavras aos seus radicais. Por exemplo, as palavras *connection*, *connected*, *connecting* serão reduzidas a \"*connect*\". Há diversos algoritmos de stemming, mas o mais famoso é o `Porter stemming`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BG9nZY_m-xK"
      },
      "source": [
        "example_words = ['connection', 'connected', 'connecting']\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "stemmed_words = [ps.stem(w) for w in example_words]\n",
        "\n",
        "print(\"Example words:\", example_words)\n",
        "print(\"Stemmed words:\", stemmed_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbHqJ4mBwwtf"
      },
      "source": [
        "O algoritmo `SnowBall` pode faz o processo de stemming em até 13 línguas diferentes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKehdjzfwoPz"
      },
      "source": [
        "print(SnowballStemmer.languages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos ver como funciona em português!\n",
        "\n",
        "**TODO:** Crie 4 listas de palavras em português contendo radicais similares (cada lista deve conter ao menos 2 palavras - preferencialmente 3 ou mais).\n",
        "\n",
        "Para cada uma das listas, gere uma outra lista com termos equivalentes \"stemizados\".\n",
        "\n",
        "Suas listas originais devem conter palavras que gerem stems do tipo: verdadeiros positivos, verdadeiros negativos, falsos positivos, falsos negativos.\n",
        "\n",
        "tp = você acha que o resultado do stemmer deve ser os mesmo para todos os termos da lista (e confirma que é em tp_stemmed)\n",
        "\n",
        "tn = mas você acha que o resultado do stemmer deve ser diferente para todos os termos da lista, embora os radicais sejam similares (e confirma que são em tn_stemmed)\n",
        "\n",
        "fp = você acha que o resultado do stemmer deve ser diferente para todos os termos da lista (mas percebe em fp_stemmed que são iguais)\n",
        "\n",
        "fn = você acha que o resultado do stemmer deve ser igual para todos os termos da lista (mas percebe em fn_stemmed que são diferentes)"
      ],
      "metadata": {
        "id": "52tj6B2OgbGE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKjqElS9xTCv"
      },
      "source": [
        "tp = #seu código aqui\n",
        "tn = #seu código aqui\n",
        "fp = #seu código aqui\n",
        "fn = #seu código aqui\n",
        "\n",
        "ss = SnowballStemmer(\"portuguese\")\n",
        "\n",
        "tp_stemmed = #seu código aqui\n",
        "tn_stemmed = #seu código aqui\n",
        "fp_stemmed = #seu código aqui\n",
        "fn_stemmed = #seu código aqui\n",
        "\n",
        "print('TP Stemmed:', tp_stemmed)\n",
        "print('TN Stemmed:', tn_stemmed)\n",
        "print('FP Stemmed:', fp_stemmed)\n",
        "print('FN Stemmed:', fn_stemmed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v-4jMBmyj35"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "Vamos comparatar a saída do Stemmer e do Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGa59T4DyjfX"
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer( )\n",
        "for item in ['am' ,'are' ,'is','was','were']:\n",
        "    print(stemmer.stem(item),end='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer( )\n",
        "for item in ['am' ,'are' ,'is','was','were']:\n",
        "    print(lemmatizer.lemmatize(item),end='\\t')"
      ],
      "metadata": {
        "id": "nJ8C2Ib6owRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esperava uma resposta diferente acima? Esperava que as saídas fossem todas \"be\" ?\n",
        "\n",
        "Esse problema acontece porque o lamatizador não sabe que estamos tratando de um verbo!"
      ],
      "metadata": {
        "id": "HooZTWn9oz2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Faça um código equivalente ao anterior (trecho da lematização), porém, passando o valor 'v' (Verbo) no parâmetro `pos` (Part-Of-Spreech).  "
      ],
      "metadata": {
        "id": "HOjsYADlpbJR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2t99kdF0Odw"
      },
      "source": [
        "#seu código aqui\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Agora faça um código equivalente ao anterior, porém, passando a constante wordnet.VERB no parâmetro `pos` (Part-Of-Spreech).  "
      ],
      "metadata": {
        "id": "yejCxHZHp4UF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#seu código aqui\n"
      ],
      "metadata": {
        "id": "Cz7jRYQ-oMhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X2KHx2JqX18"
      },
      "source": [
        "### POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conjunto de Tags Baseado no [Penn Treebank Tag Set](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
      ],
      "metadata": {
        "id": "oPUnW1TmKxVS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcoQOxHepioT"
      },
      "source": [
        "sent = \"Albert Einstein was born in Ulm, Germany, in 1879.\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sent)\n",
        "print('Sentence:', tokens)\n",
        "print(nltk.pos_tag(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Formule uma frase (mesmo que não faça sentido) de forma que o POS Tagger classifique 'Albert' como verbo."
      ],
      "metadata": {
        "id": "IK54GFOjq9KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#seu código aqui\n"
      ],
      "metadata": {
        "id": "FiGmo31Yqoc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Agora formule duas sentenças (que façam sentido) onde a mesma palavra é classificada de uma forma na sentença 1 e de outra forma na sentença 2."
      ],
      "metadata": {
        "id": "LaQbBRBMsF_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#seu código aqui\n"
      ],
      "metadata": {
        "id": "9zLgX7jCLcPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization com POS-Tagging\n",
        "\n",
        "Vamos automatizar o processo de lematização: vamos detectar a POS-TAG com nltk.pos_tag e então passá-la para o wordnet lemmatizer."
      ],
      "metadata": {
        "id": "bm_U2fHYtfoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute o comando abaixo para criar as classes necessárias. Analise o código para entender.\n"
      ],
      "metadata": {
        "id": "On3xqchMU0eK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# observe e o que o código abaixo faz\n",
        "\n",
        "class Splitter(object):\n",
        "    \"\"\"\n",
        "    split the document into sentences and tokenize each sentence\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
        "\n",
        "    def split(self,text):\n",
        "        \"\"\"\n",
        "        out : ['What', 'can', 'I', 'say', 'about', 'this', 'place', '.']\n",
        "        \"\"\"\n",
        "        # split into single sentence\n",
        "        sentences = self.splitter.tokenize(text)\n",
        "        # tokenization in each sentences\n",
        "        tokens = [self.tokenizer.tokenize(sent) for sent in sentences]\n",
        "        return tokens\n",
        "\n",
        "\n",
        "class LemmatizationWithPOSTagger(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def get_wordnet_pos(self,treebank_tag):\n",
        "        \"\"\"\n",
        "        return WORDNET POS compliance to WORDNET lemmatization (a,n,r,v)\n",
        "        \"\"\"\n",
        "        if treebank_tag.startswith('A'):\n",
        "            return wordnet.ADJ\n",
        "        elif treebank_tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif treebank_tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif treebank_tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            # As default pos in lemmatization is Noun\n",
        "            return wordnet.NOUN\n",
        "\n",
        "    def pos_tag_lemma_complete(self,tokens):\n",
        "        # find the pos tag for each token [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....\n",
        "        pos_tokens = [nltk.pos_tag(token) for token in tokens]\n",
        "\n",
        "        # lemmatization using pos tag\n",
        "        # convert into feature set of [('What', 'What', ['WP']), ('can', 'can', ['MD']), ... ie [original WORD, Lemmatized word, POS tag]\n",
        "        pos_tokens = [ [(word, lemmatizer.lemmatize(word,self.get_wordnet_pos(pos_tag)), [pos_tag]) for (word,pos_tag) in pos] for pos in pos_tokens]\n",
        "        return pos_tokens\n",
        "\n",
        "    def pos_tag_lemma_basic(self,tokens):\n",
        "        # find the pos tag for each token [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....\n",
        "        pos_tokens = [nltk.pos_tag(token) for token in tokens]\n",
        "\n",
        "        # lemmatization using pos tag\n",
        "        # convert into list of Lemmatized words\n",
        "        pos_tokens = [ [lemmatizer.lemmatize(word,self.get_wordnet_pos(pos_tag)) for (word,pos_tag) in pos] for pos in pos_tokens]\n",
        "\n",
        "        return pos_tokens\n"
      ],
      "metadata": {
        "id": "jryky1nwuzHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora complete o código abaixo com o que se pede:"
      ],
      "metadata": {
        "id": "G2rY0e1nVDMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#seu código aqui\n",
        "# instancie um objeto de cada classe\n",
        "\n",
        "#seu código aqui\n",
        "# crie um pequeno texto de exemplo\n",
        "text = '...'\n",
        "\n",
        "#seu código aqui\n",
        "#passo 1 - use o splitter e imprima o resultado\n",
        "\n",
        "\n",
        "#seu código aqui\n",
        "#passo 2 - use o lematizador básico e imprima o resultado\n",
        "\n",
        "\n",
        "#seu código aqui\n",
        "#passo 3 - use o lematizador completo e imprima o resultado\n"
      ],
      "metadata": {
        "id": "-eAtWTAyVYEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outras Bibliotecas\n",
        "\n",
        "Vamos conhecer outras libs que nos ajudam nesse tipo de tarefa"
      ],
      "metadata": {
        "id": "wNV1AtI_h0MI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lematização usando [Stanza](https://stanfordnlp.github.io/stanza/)"
      ],
      "metadata": {
        "id": "_DhUQkZ0hExT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stanza"
      ],
      "metadata": {
        "id": "jb0YxceIg62A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** complete o código abaixo"
      ],
      "metadata": {
        "id": "kVWa_S8emw_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma')\n",
        "\n",
        "# Seu código aqui\n",
        "# Substitua a string vazia por uma sequencia de termos separados por espaço)\n",
        "doc = nlp('')\n",
        "\n",
        "# Seu código aqui\n",
        "# Imprima lado a lado os termos com seus repectivos lemas\n",
        "# Em uma única expressão usando list comprehension, gere o lemas para os termos acima, manipulando doc.sentences, doc.sentences.words\n",
        "# O texto e o lema podem ser obtidos com .text e .lemma em cada item (cada word em doc.sentences.words)\n"
      ],
      "metadata": {
        "id": "VDo9OWUSgZyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lematização usando [Spacy](https://spacy.io) - [lemminflect](https://spacy.io/universe/project/lemminflect/)"
      ],
      "metadata": {
        "id": "vIyPvAGsfwL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lemminflect"
      ],
      "metadata": {
        "id": "e6pePqLRfCku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Complete o código"
      ],
      "metadata": {
        "id": "Yj3ifu-kqFTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import lemminflect\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp('best well better be was were is am')\n",
        "\n",
        "# aqui podemos iterar em 'doc'\n",
        "print(f\"{'Text':{8}} | {'Lemma':{6}}\\n\")\n",
        "for token in doc:\n",
        "    print(...) # seu código aqui (complete o código para montar a tabela)\n"
      ],
      "metadata": {
        "id": "1tHHf-n34yVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Função getLemma"
      ],
      "metadata": {
        "id": "Chg9BtfHSvZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lemminflect import getLemma\n",
        "getLemma( 'watches', upos='VERB')"
      ],
      "metadata": {
        "id": "KZ_gxxmrSuIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-Gramas"
      ],
      "metadata": {
        "id": "xyTf8DinROvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementação ad-hoc (função que gera os n-gramas a partir da lista de tokens)"
      ],
      "metadata": {
        "id": "uKDfdEzlRXJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def n_grams(tokens,ngram=1):\n",
        "  temp=zip(*[tokens[i:] for i in range(0,ngram)])\n",
        "  result=[' '.join(ngram) for ngram in temp]\n",
        "  return result"
      ],
      "metadata": {
        "id": "ZUTgVtW2RVli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Produza um exemplo para um texto qualquer. Primeiro, defina uma forma de tokenizar o texto e depois gere os n-gramas que desejar com a função acima."
      ],
      "metadata": {
        "id": "2L6_J56LTDJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# seu código aqui\n",
        "\n",
        "text = '''seu texto'''"
      ],
      "metadata": {
        "id": "-sMoCrzMR5YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usando NLTK"
      ],
      "metadata": {
        "id": "dfGvkZuPSptY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe o código a seguir:"
      ],
      "metadata": {
        "id": "Ln21cO9ejSon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "text = '''Hello Mr. Smith, how are you doing today?\n",
        "    The weather is great, and city is awesome.\n",
        "    The sky is pinkish-blue. You shouldn't eat cardboard\n",
        "'''\n",
        "\n",
        "tokens = [word.lower() for sent in sent_tokenize(text) for word in word_tokenize(sent) if word.isalnum()]\n",
        "\n",
        "list(ngrams(tokens, 3))"
      ],
      "metadata": {
        "id": "wP8YN89WT9bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A saída do código acima não é igual à nossa implementação ad-hoc. O código abaixo torna a saída igual à nossa (o nltk não retorna a string, mas sim o zip). Adicione uma linha de código no final, apenas para imprimir o resultado."
      ],
      "metadata": {
        "id": "GKuDJNXTi-7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp=ngrams(tokens, 3)\n",
        "result=[' '.join(ngram) for ngram in temp]\n",
        "# seu código aqui"
      ],
      "metadata": {
        "id": "HIhz8cKgi_eG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}